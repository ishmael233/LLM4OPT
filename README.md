# LLM4OPT: from Modeling to Solving

## LLMs for Optimization

Optimization techniques have become a core tool for solving complex problems across diverse fields such as engineering design, economic planning, and scientific discovery. When tackling real-world challenges, researchers typically abstract them into mathematical optimization models, which are then solved using various optimization algorithms. In recent years, large language models (LLMs) have emerged as a powerful paradigm, offering novel perspectives and capabilities for the field of optimization. The application of LLMs in optimization spans two primary areas: **modeling** and **solving**.

We provide a **classification** and **collection** of these applications to serve as a reference for researchers. We propose a new taxonomy to clearly articulate the deep relationship between LLMs and the optimization process. Specifically, we classify the research in this field into two main categories: LLMs for optimization modeling and LLMs for optimization solving. The latter, LLMs for optimization solving, is further subdivided into three classes: LLMs as optimizers, low-level LLMs assisted optimization algorithms, and high-level LLMs assisted optimization algorithms. The specific definitions for these are as follows:


1. **LLMs for Optimization Modeling** aims to automatically transform unstructured natural language descriptions of problems into mathematical optimization models that can be understood and solved by machines. This research area is the foundational step toward achieving a fully LLM-driven optimization workflow, with the core challenge lying in bridging the gap between ambiguous natural language and precise mathematical models. Two main paradigms exist in this field: prompt-based and learning-based methods. The former typically relies on carefully designed prompting procedures or the collaboration of multiple agents, with the advantage of being quickly deployable without additional training. The latter, however, often requires data synthesis and model fine-tuning. While these methods incur training costs, they can significantly enhance modeling reliability in complex scenarios.

2. **LLMs as Optimizers** refers to the use of LLMs directly as general-purpose optimizers, which solve optimization problems through iterative natural language interaction without relying on traditional optimization algorithm frameworks. This paradigm leverages the powerful in-context learning and complex reasoning capabilities of LLMs to analyze optimization trajectories and continuously generate more promising candidate solutions. Although this direct optimization approach faces challenges such as model dependency, it stands as the earliest bridge connecting large models to optimization tasks, possessing significant pioneering importance and potential.

3. **Low-level LLMs for Optimization Algorithms** refers to the paradigm of embedding LLMs as intelligent components within traditional optimization algorithms to enhance their specific operations. Unlike acting as a standalone optimizer, this approach tightly integrates LLMs with traditional evolutionary algorithms, leveraging the models' powerful data analysis capabilities and rich domain knowledge to boost algorithmic performance. Specifically, LLMs can be applied to core processes such as algorithm initialization, evolutionary operators, algorithm configuration, and fitness evaluation. This method relies on an efficient optimization framework to empower traditional components, enabling them to adapt more intelligently to different problem characteristics and search states, thereby improving the algorithm's efficiency and solution quality at a fundamental level.

4. **High-level LLMs for Optimization Algorithms** differs from the low-level assistance paradigm by focusing on the top-level orchestration or design of algorithms, rather than their internal components. Specifically, high-level assistance involves two types of tasks: algorithm selection and algorithm generation. The algorithm selection task aims to match the most suitable algorithm from a pool for different problem instances, whereas algorithm generation goes a step further, requiring LLMs to autonomously design new algorithms to better fit a given problem. This class of methods grants LLMs a global perspective over the entire optimization task, transforming their role from an internal assistant to a top-level designer for the entire optimization workflow.

## How to contribute


## Table of Contents

- ðŸ“š [Survey Papers](#1-survey-papers)
- ðŸ¤– [LLMs for Optimization Modeling](#2-LLMs-for-Optimization-Modeling)
  - [Prompt-based Methods](#21-Prompt-based-Methods)
  - [Learning-based Methods](#22-Learning-based-Methods)
- ðŸ’» [LLMs for Optimization Solving](#3-LLMs-for-Optimization-Solving)
  - [LLMs as Optimizers](#31-LLMs-as-Optimizers)
  - [Low-level LLMs for Optimization Algorithms](#32-Low-level-LLMs-for-Optimization-Algorithms)
  - [High-level LLMs for Optimization Algorithms](#33-High-level-LLMs-for-Optimization-Algorithms)


## 1. Survey Papers
| **Title** | **Publication** | **Year** |
|-----------|--------------|------|
| [Evolutionary Computation in the Era of Large Language Model: Survey and Roadmap](https://arxiv.org/abs/2401.10034) | TEVC | 2024 |
| [When Large Language Model Meets Optimization](https://arxiv.org/abs/2405.10098) | SWEC | 2024 |
| [When Large Language Models Meet Evolutionary Algorithms](https://arxiv.org/abs/2401.10510) | Research | 2024 |
| [Exploring the Improvement of Evolutionary Computation via Large Language Models](https://arxiv.org/abs/2405.02876) | GECCO | 2024 |
| [A Systematic Survey on Large Language Models for Algorithm Design](https://arxiv.org/abs/2410.14716) | ArXiv | 2024 |
| [Deep Insights into Automated Optimization with Large Language Models and Evolutionary Algorithms](https://arxiv.org/abs/2410.20848) | ArXiv | 2024 |
| [Toward Automated Algorithm Design: A Survey and Practical Guide to Meta-Black-Box-Optimization](https://arxiv.org/abs/2411.00625) | TEVC | 2024 |
| [Evolutionary Computation and Large Language Models: A Survey of Methods, Synergies, and Applications](https://arxiv.org/abs/2505.15741) | ArXiv | 2025|
| [A Survey of Optimization Modeling Meets LLMs: Progress and Future Directions](https://arxiv.org/abs/2508.10047) | ArXiv | 2025 |


## 2. LLMs for Optimization Modeling
We'll delve into two core paradigms for using LLMs for optmization modeling: prompt-based methods and learning-based methods. Prompt-based methods leverage carefully designed instructions to guide the LLM in completing modeling tasks. This often involves a two-stage workflow or the collaboration of multiple LLMs. Learning-based methods, on the other hand, use fine-tuned models to directly generate mathematical models. However, they typically require pre-training on a large amount of synthetic data.

<div style="text-align: center;">
    <img src="fig/formulation.png" width="600" />
</div>

### 2.1 Prompt-based Methods

| **Title** | **Publication** | **Year** | **Type** | **Summary** |
|-----------|--------------|------|------|----------|
| [x](xx) | CPAIOR | 2023 | Two-stage | Fine-tune models for named entity recognition by incorporating traditional NLP methods.
| [x](xx) | GECCO | 2023 | Direct | Directly leverage LLMs to generate mathematical models.
| [x](xx) | arXiv | 2023 |Two-stage | Embed LLMs within the two-stage framework.
| [x](xx) | arXiv | 2023 | Two-stage | Employ fine-tuned models for constraint classiffcation.
| [x](xx) | ICLR | 2023 | Multi-agent | Construct dynamic reasoning chains using 11 expert agents.
| [x](xx) | ICML | 2023 | Multi-agent | Develop a conductor agent to coordinate modeling, programming, and evaluation processes.
| [x](xx) | INFOR | 2024 | Multi-agent | Develop inter-agent cross-validation to replace solver-dependent veriffcation.
| [x](xx) | SGC | 2024 | Two-stage | Apply the two-stage framework to energy management systems.
| [x](xx) | NeurIPS | 2024 | Two-stage | Enhance modeling performance through code-based formalization.
| [x](xx) | NAACL | 2024 | Two-stage | Develop the MAMO benchmark with ordinary differential equation extensions.
| [x](xx) | arXiv | 2024 | Two-stage | Predefine abstract structural constraints to regulate the LLM outputs.
| [x](xx) | arXiv | 2024 | Two-stage | Apply the two-stage framework to travel planning.
| [x](xx) | arXiv | 2024 | Interactive | Support both single-input and interactive-input modes.
| [x](xx) | arXiv | 2024 | Multi-agent | Introduce self-corrective prompts and structure-aware modeling based on OptiMUS.
| [x](xx) | TiiS | 2024 | Interactive | Develop an interactive system for user input processing with ffve selectable task options.
| [x](xx) | CUI | 2025 | Interactive | Convert user priorities into optimization constraints during dialog interactions.
| [x](xx) | arXiv | 2025 | Two-stage | Conduct hierarchical Monte Carlo tree search over the hypothesis space.
| [x](xx) | arXiv | 2025 | Two-stage | Generate variable mapping functions via LLMs with lightweight verification.
| [x](xx) | RG | 2025 | Multi-agent | Deploy multiple independent reviewers to evaluate modeling results.
| [x](xx) | arXiv | 2025 | Multi-agent | Replace the conductor agent with structured, predictable workffows.



| **Title** | **Publication** | **Year** | **Summary** |
|-----------|--------------|------|----------|
| [Large Language Models as Optimizers](https://doi.org/10.48550/arXiv.2309.03409) | ICLR | Sep 2023 | The study introduces "Optimization by PROmpting (OPRO)," a method that utilizes Large Language Models (LLMs) as optimizers. In each optimization step, the LLM generates new solutions based on a prompt containing previously generated solutions and their values. These new solutions are then evaluated and added to the prompt for the next iteration. |
| [Towards Optimizing with Large Language Models](https://arxiv.org/abs/2310.05204) | KDD | Oct 2023 | This study demonstrates that large language models (LLMs) can effectively perform iterative optimization tasksâ€”including gradient descent, hill climbing, grid search, and black-box optimizationâ€”through interactive prompting, achieving strong performance in small-scale problems but exhibiting sensitivity to problem dimensionality and value ranges. |
| [Large Language Model Agent for Hyper-Parameter Optimization](https://arxiv.org/abs/2402.01881) | ArXiv | Feb 2024 | This paper introduces AgentHPO, an LLM-driven paradigm for automating hyperparameter optimization, reducing trial complexity, and enhancing interpretability compared to traditional AutoML methods. Empirical results across 12 machine learning tasks demonstrate that AgentHPO not only matches but often surpasses human performance while providing explainable optimization strategies. |
| [Large Language Models As Evolution Strategies](https://dl.acm.org/doi/abs/10.1145/3638530.3654238) | GECCO | Feb 2024 | This work explores the zero-shot capability of large language models (LLMs) in black-box optimization, introducing EvoLLMâ€”a novel evolution strategy leveraging LLMs as in-context recombination operators. Empirical results demonstrate that EvoLLM outperforms baseline algorithms, with performance further enhanced through instruction fine-tuning on teacher optimization trajectories. |
| [Exploring the True Potential: Evaluating the Black-box Optimization Capability of Large Language Models](https://arxiv.org/abs/2404.06290) | ArXiv | Apr 2024 | This paper systematically evaluates the potential of large language models (LLMs) in black-box optimization, revealing both their limitations in pure numerical tasks and their strengths in broader optimization contexts. The findings highlight LLMsâ€™ ability to leverage heuristics from prompts, paving the way for future applications in diverse optimization scenarios. |
| [Revisiting OPRO: The Limitations of Small-Scale LLMs as Optimizers](https://arxiv.org/abs/2405.10276) | ACL Finding | May 2024 | This paper demonstrates that small-scale LLMs (e.g., LLaMa-2, Mistral 7B) lack sufficient self-optimization capabilities to effectively leverage OPRO, leading to marginal performance gains compared to traditional CoT prompts, and recommends prioritizing direct, explicit instructions over automated optimization for efficient prompt engineering. |
| [LLMs can Schedule](https://arxiv.org/abs/2408.06993) | ArXiv | Aug 2024 | This paper introduces the first supervised 120k natural language dataset for training LLMs on job shop scheduling problems (JSSP), demonstrating that fine-tuned LLMs (e.g., Phi-3) achieve comparable performance to neural methods on small-scale JSSP tasks, with a proposed sampling strategy enhancing solution quality. The work bridges LLMs' reasoning capabilities to combinatorial optimization, offering a novel data-driven framework for production scheduling. |
| [Large Language Model-Based Wireless Network Design](https://doi.org/10.1109/LWC.2024.3462556) | LWC | Sep 2024 | The paper proposes LMCO, an LLM-based combinatorial optimization framework for wireless network planning, which autonomously determines the number and placement of access points by integrating natural language prompts, domain knowledge, and ray-tracing simulations. Experimental results demonstrate that LMCO significantly outperforms traditional methods like ACO in both efficiency and adaptability, especially in complex scenarios, showcasing the transformative potential of LLMs in combinatorial optimization tasks. |
| [Large Language Models for Combinatorial Optimization of Design Structure Matrix](https://arxiv.org/pdf/2411.12571) | ArXiv | Nov 2024 | This study proposes an LLM-based framework integrating domain knowledge and network topology for combinatorial optimization of Design Structure Matrix (DSM), demonstrating superior convergence speed and solution quality over benchmarks. The results highlight that semantic reasoning with contextual knowledge significantly enhances LLMs' capability in solving real-world engineering CO problems. |
| [Trace is the Next AutoDiff: Generative Optimization with Rich Feedback, Execution Traces, and LLMs](https://arxiv.org/pdf/2406.16218) | ArXiv | Nov 2024 | The paper proposes â€‹Trace, a framework for end-to-end optimization of non-differentiable computational workflows using execution traces and LLMs. By reformulating optimization as an OPTO problem and leveraging LLM-based optimizer OptoPrime, Trace efficiently tunes heterogeneous parameters (code, prompts, hyperparameters) with rich feedback, outperforming domain-specific methods in tasks like robot control and prompt engineering. |
| [Pretrained Optimization Model for Zero-Shot Black Box Optimization](https://proceedings.neurips.cc/paper_files/paper/2024/file/19e9a88d91917775b34fdad447ed8908-Paper-Conference.pdf) | NeurIPS | Dec 2024 | POM (Pretrained Optimization Model) achieves state-of-the-art zero-shot black-box optimization by meta-training on diverse tasks via gradient-based learning, outperforming CMA-ES and L-SHADE in high-dimensional scenarios through adaptive mutation/crossover modules and mask-enhanced exploration. It enables efficient few-shot adaptation with minimal samples, demonstrating robust generalization across dimensions and task distributions.|
| [An Optimizable Suffix Is Worth A Thousand Templates: Efficient Black-box Jailbreaking without Affirmative Phrases via LLM as Optimizer](https://arxiv.org/pdf/2408.11313) |  NAACL finding | Apr 2025 | The paper introduces â€‹â€‹ECLIPSEâ€‹â€‹, a â€‹â€‹black-box jailbreaking methodâ€‹â€‹ that uses â€‹â€‹LLMs as optimizersâ€‹â€‹ to generate adversarial suffixes, bypassing safety alignments without predefined affirmative phrases. It achieves â€‹â€‹92% attack success rateâ€‹â€‹ across models like GPT-3.5, outperforming prior methods in efficiency and naturalness while eliminating white-box access needs.|
| [Generalists vs. Specialists: Evaluating LLMs on Highly-Constrained Biophysical Sequence Optimization Tasks](https://arxiv.org/pdf/2410.22296?) |  ArXiv | Apr 2025 | The paper introduces â€‹â€‹Ehrlich functionsâ€‹â€‹, a synthetic benchmark for constrained biophysical sequence optimization, and proposes â€‹â€‹LLOMEâ€‹â€‹, a bilevel LLM-based optimization method with a novel â€‹â€‹Margin-Aligned Expectation (MargE)â€‹â€‹ loss, demonstrating that LLMs can outperform specialized solvers (e.g., LaMBO-2) on moderately difficult tasks while remaining competitive on easy/hard variants.|



#### 1.2 Multimodal LLMs as Optimizers


<div style="text-align: center;">
    <img src="fig/direct.png" width="200" />
</div>

**Title**                                                                                                                                | **Publication** | **Year**   | **Summary**                                                                                                                                                                                                                   |
|------------------------------------------------------------------------------------------------------------------------------------------|:----------------:|:----------:|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| [How Multimodal Integration Boost the Performance of LLM for Optimization: Case Study on Capacitated Vehicle Routing Problems](https://doi.org/10.48550/arXiv.2403.01757) |     ArXiv       |  Mar 2024  | This paper presents a multimodal LLM-based optimization framework that combines textual and visual prompts to better capture relationships among decision variables, improving performance on high-dimensional optimization problems like the capacitated vehicle routing problem (CVRP). |
| [Eyeballing Combinatorial Problems: A Case Study of Using Multimodal Large Language Models to Solve Traveling Salesman Problems](https://arxiv.org/abs/2406.06865) |     ArXiv       |  Jun 2024  | This paper explores the use of Multimodal Large Language Models (MLLMs) to solve the Traveling Salesman Problem (TSP) by leveraging their visual capabilities to analyze images of point distributions on a 2D plane.                         |
| [Visual Reasoning and Multi-Agent Approach in Multimodal Large Language Models (MLLMs): Solving TSP and mTSP Combinatorial Challenges](https://arxiv.org/abs/2407.00092) |     ArXiv       |  Jun 2024  | This study explores how Multimodal Large Language Models (MLLMs) solve the Traveling Salesman Problem (TSP) and Multiple TSP using images of point distributions. Two multi-agent models were tested: Multi-Agent 1, which refined solutions, and Multi-Agent 2, which focused on rapid decision-making. |
| [Bridging Visualization and Optimization: Multimodal Large Language Models on Graph-Structured Combinatorial Optimization](https://arxiv.org/abs/2501.11968) |     ArXiv       |  Jan 2025  | This study proposes transforming graph-structured combinatorial problems into images to better capture their structural features, enabling machines to process them in a way similar to human spatial reasoning. By combining multimodal large language models (MLLMs) with simple search techniques, the approach effectively addresses various graph-based tasks, such as influence maximization and network dismantling. |


### 2. Low-level LLM-assisted Optimization Algorithms
#### 2.1 LLMs as Operators
| **Title** | **Publication** | **Year** | **Summary** |
|-----------|---------------|----------|------------|
| [Large language model-based evolutionary optimizer: Reasoning with elitism](https://arxiv.org/pdf/2403.02054) | Neurocomputing | Mar 2024 | The paper proposes LEO, a population-based evolutionary optimizer leveraging large language models (LLMs) for zero-shot numerical optimization. By integrating exploration-exploitation pools and prompt engineering, LEO achieves competitive performance in multi-objective and industrial problems while addressing challenges like hallucination through jitter and identity-driven context prompts. |
| [LLM Guided Evolution - The Automation of Models Advancing Models](https://arxiv.org/pdf/2403.11446) | GECCO | Mar 2024 | The paper introduces â€‹â€‹Guided Evolution (GE)â€‹â€‹, a framework that uses â€‹â€‹LLMs to autonomously optimize neural architecturesâ€‹â€‹ through code mutations and feedback-driven â€‹â€‹Evolution of Thought (EoT)â€‹â€‹, achieving improved accuracy (e.g., â€‹â€‹93.34% on CIFAR-10â€‹â€‹) without increasing model size. |
| [Large Language Models as Evolutionary Optimizers](https://arxiv.org/pdf/2310.19046) | CEC | Apr 2024 | The study proposes LMEA, a novel framework that employs large language models as evolutionary optimizers for combinatorial problems. By leveraging zero-shot prompting and adaptive temperature control, LMEA achieves competitive performance on TSP instances (up to 20 nodes) without domain-specific operator design or model fine-tuning, demonstrating the potential of LLMs in automated optimization. |
| [Language Model Crossover: Variation through Few-Shot Prompting](https://doi.org/10.1145/3694791) | ACM Transactions on EL | May 2024 | The paper introduces Language Model Crossover (LMX), a novel evolutionary operator leveraging large language models' few-shot prompting to generate semantically-rich offspring by pattern completion. LMX enables domain-agnostic text-based evolution across binary strings, equations, code, and image prompts, demonstrating its versatility and scalability with model size. |
| [Advancing Automated Knowledge Transfer in Evolutionary Multitasking via Large Language Models](https://arxiv.org/pdf/2409.04270) | ArXiv | Sep 2024 | This study proposes an LLM-based framework (LLMOF) that autonomously generates knowledge transfer models for evolutionary multitask optimization, leveraging few-shot chain-of-thought prompting and multi-objective evolutionary mechanisms to balance effectiveness and efficiency. Experimental results demonstrate LLMOF's superiority over hand-crafted models, achieving higher-quality solutions with reduced computational costs across diverse optimization benchmarks. |
| [Large Language Model Implemented Simulated Annealing Algorithm for Traveling Salesman Problem](https://doi.org/10.1109/SMC54092.2024.10831145) | SMC | Oct 2024 | This paper proposes a novel approach combining large language models (LLMs) with simulated annealing (SA) to solve the Traveling Salesman Problem (TSP), demonstrating that LLMs can effectively guide heuristic algorithms while maintaining accessibility for non-experts. The results show competitive performance on small-to-medium TSP instances but highlight scalability limitations for larger problems. |
| [LLM-Virus: Evolutionary Jailbreak Attack on Large Language Models](https://arxiv.org/pdf/2501.00055) | ArXiv | Dec 2024 | LLM-Virus proposes an evolutionary jailbreak attack framework inspired by viral infection dynamics, leveraging LLMs as heuristic evolutionary operators to optimize adversarial templates. By integrating transfer learning and LLM-enhanced crossover/mutation, it achieves state-of-the-art attack success rates (e.g., 90.8% on GPT-3.5) with significantly reduced computational costs, demonstrating superior transferability and efficiency compared to gradient-based and rule-based methods. |
| [Can Large Language Models Be Trusted as Black-Box Evolutionary Optimizers for Combinatorial Problems?](https://arxiv.org/pdf/2501.15081) | ArXiv | Jan 2025 | This study rigorously evaluates LLMs as black-box evolutionary optimizers for combinatorial problems, demonstrating their effectiveness in selection, crossover, and mutation stages but limitations in data-intensive tasks like initialization. It introduces population-level optimization and error-correction mechanisms, showing cost efficiency and improved reliability with models like GPT-4, while highlighting scalability challenges for large datasets. |
| [PAIR: A Novel Large Language Model-Guided Selection Strategy for Evolutionary Algorithms](https://arxiv.org/abs/2503.03239) | ArXiv | Mar 2025 | The paper introduces â€‹â€‹PAIRâ€‹â€‹, a novel selection strategy for evolutionary algorithms that leverages â€‹â€‹large language models (LLMs)â€‹â€‹ to intelligently pair individuals based on genetic diversity, fitness, and compatibility, significantly improving optimization performance on problems like the Traveling Salesman Problem (TSP). |
| [LLM-Guided Evolution: An Autonomous Model Optimization for Object Detection](https://arxiv.org/pdf/2504.02280) | GECCO | Apr 2025 | The paper introduces â€‹â€‹LLM-Guided Evolution (LLM-GE)â€‹â€‹, a framework that leverages large language models (LLMs) to autonomously optimize YOLO architectures for object detection on the KITTI dataset, achieving improved accuracy (e.g., mAP@50 from 92.5% to 94.5%) through code-aware mutations and evolutionary feedback. |
| [Large Language Models as Particle Swarm Optimizers](https://arxiv.org/pdf/2504.09247) | ArXiv | Apr 2025 | This paper introduces â€‹â€‹LMPSOâ€‹â€‹, a novel method that integrates â€‹â€‹large language models (LLMs)â€‹â€‹ into â€‹â€‹Particle Swarm Optimization (PSO)â€‹â€‹, enabling LLMs to generate structured solutions (e.g., TSP paths, math expressions) through natural language prompts, while maintaining PSOâ€™s collaborative search framework. |



#### 2.2 LLMs driven parameter setting problems
| **Title** | **Publication** | **Year** | **Summary** |
|-----------|--------------|------|----------|
| **Parameter Control** |
| [An investigation on the use of Large Language Models for hyperparameter tuning in Evolutionary Algorithms](https://dl.acm.org/doi/abs/10.1145/3638530.3664163) | GECCO | Aug 2024 | This study demonstrates the feasibility of using large language models (LLMs) like Llama2-70b for real-time hyperparameter tuning in evolutionary algorithms, specifically step-size adaptation in (1+1)-ES. Experimental results on BBOB benchmarks show that LLM-based strategies can rival traditional methods (e.g., one-fifth success rule), highlighting their potential as reasoning-driven optimizers for adaptive parameter control in complex optimization landscapes. |
| [LLaMA Tunes CMA-ES](https://www.esann.org/sites/default/files/proceedings/2024/ES2024-136.pdf) | ESANN | Oct 2024 | The paper proposes LLaMA-ES, a method that leverages the 70B-parameter LLaMA3 model to dynamically control CMA-ES hyperparameters through iterative analysis of optimization history. Experimental results on benchmark functions demonstrate significant performance improvements, showcasing the potential of LLMs in automating evolutionary algorithm adaptation. |
| [Metaheuristics and Large Language Models Join Forces: Toward an Integrated Optimization Approach](https://doi.org/10.1109/ACCESS.2024.3524176) | IEEE Access | Dec 2024 | This study proposes a novel integration of Large Language Models (LLMs) as pattern recognition tools within metaheuristics, enhancing combinatorial optimization by leveraging LLM-generated parameters (alpha and beta values) to guide node selection. The approach, validated on multi-hop influence maximization in social networks, demonstrates superior performance over existing methods, particularly for large-scale instances, by effectively combining LLM-driven insights with metaheuristic search. |
| **Parameter Tuning**  |
| [Large Language Models for Tuning Evolution Strategies](https://doi.org/10.48550/arXiv.2405.10999) | ArXiv | May 2024 | This paper proposes a feedback loop framework leveraging Large Language Models (LLMs) to autonomously tune Evolution Strategies (ES) parameters. By iteratively generating code, executing experiments, and analyzing results via LLMs (e.g., LLaMA3), the method effectively optimizes key parameters like Ï„, demonstrating improved performance on benchmark functions such as the 5D Sphere. The approach highlights LLMsâ€™ potential in automating algorithm refinement and enabling self-adaptive optimization systems. |


#### 2.3 LLMs as Operators for Multi-objective Optimization
| **Title** | **Publication** | **Year** | **Summary** |
|-----------|--------------|------|----------|
| [Large Language Model for Multi-objective Evolutionary Optimization](https://arxiv.org/abs/2310.12541) | EMO | Oct 2023 | This study pioneers the use of large language models (LLMs) as zero-shot optimizers in multi-objective evolutionary optimization, decomposing problems into single-objective subproblems for LLM-driven solution generation without problem-specific training, and introduces a white-box linear operator inspired by LLM behavior to enhance efficiency and generalization across diverse optimization tasks. |
| [Large Language Model-Aided Evolutionary Search for Constrained Multiobjective Optimization](https://doi.org/10.1007/978-981-97-5581-3_18) | ICIC | May 2024 | This study integrates LLMs with evolutionary algorithms to solve constrained multi-objective optimization problems, where the LLM acts as a learnable search operator guided by tailored prompts incorporating objectives and constraints. Experimental results demonstrate that LLM-aided evolution significantly accelerates population convergence while maintaining diversity, outperforming state-of-the-art methods. |
| [Efficient Evolutionary Search Over Chemical Space with Large Language Models](https://arxiv.org/abs/2406.16976) â€” Molecular Design | ArXiv | Jun 2024 | MoLLEO integrates chemistry-aware large language models (LLMs) into evolutionary algorithms, acting as informed genetic operators to enhance molecular optimization. By leveraging LLMs like GPT-4 and BioT5 for crossover and mutation, it achieves superior performance in drug discovery tasks with fewer oracle calls, outperforming traditional methods in both single- and multi-objective settings. |
| [Large Language Model Aided Multi-objective Evolutionary Algorithm: a Low-cost Adaptive Approach](https://doi.org/10.48550/arXiv.2410.02301) | ArXiv | Oct 2024 | This study proposes a low-cost adaptive framework integrating large language models (LLMs) with multi-objective evolutionary algorithms (MOEAs), where LLMs act as black-box optimizers guided by elite solutions and structured prompts. The hybrid mechanism minimizes LLM interactions while leveraging its reasoning to accelerate convergence and enhance diversity, achieving superior performance on benchmark problems compared to state-of-the-art MOEAs. |
| [Large Language Model Based Multi-Objective Optimization for Integrated Sensing and Communications in UAV Networks](https://doi.org/10.1109/LWC.2025.3529082) | LWC | Oct 2024 | This study proposes LEDMA, an LLM-enabled evolutionary algorithm, to balance communication and sensing in UAV-enabled ISAC networks by jointly optimizing deployment and power control. By integrating LLMs as search operators with problem-specific prompts and decomposition-based optimization, LEDMA outperforms baseline methods in Pareto front quality and convergence, demonstrating the potential of LLMs in solving complex engineering MOPs. |
| [MOLLM: Multi-Objective Large Language Model for Molecular Design â€“ Optimizing with Experts](https://arxiv.org/pdf/2502.12845) â€” Molecular Design | ArXiv | Feb 2025 | MOLLM is a novel LLM-driven framework for multi-objective molecular design, integrating in-context learning and genetic algorithms to optimize properties like drug-likeness and synthetic accessibility without task-specific training. It leverages LLMs as crossover/mutation operators and achieves state-of-the-art performance in efficiency and optimization quality, particularly excelling in complex scenarios with 5-6 objectives. |
| [Language Model Evolutionary Algorithms for Recommender Systems: Benchmarks and Algorithm Comparisons](https://arxiv.org/pdf/2411.10697) | ArXiv | Mar 2025 | This paper introduces LLM-enhanced evolutionary algorithms for optimizing recommendation prompts, demonstrating their potential through the RSBench benchmark while highlighting the synergy between large language models and evolutionary computation in multi-objective recommendation systems. |
| [An LLM-Empowered Adaptive Evolutionary Algorithm For Multi-Component Deep Learning Systems](https://doi.org/10.1609/aaai.v39i19.34303) | AAAI | Apr 2025 | This paper proposes â€‹â€‹Î¼MOEAâ€‹â€‹, an LLM-enhanced evolutionary algorithm that leverages GPT-4's language understanding and reasoning to generate high-quality initial populations and escape local optima, significantly improving efficiency and diversity in detecting safety violations of multi-component deep learning systems (e.g., autonomous driving). |




#### 2.4 LLMs as Surrogate Models
| **Title** | **Publication** | **Year** | **Summary** |
|-----------|---------------|----------|------------|
| [Large Language Models as Surrogate Models in Evolutionary Algorithms: A Preliminary Study](https://arxiv.org/pdf/2406.10675) | SEC | Jun 2024 | This study proposes a novel LLM-based surrogate model for evolutionary algorithms, leveraging LLMs' inference capabilities to predict solution quality via zero-shot classification and regression tasks without training. Experimental results demonstrate that LLMs (e.g., Llama3 and Mixtral) achieve competitive performance compared to traditional surrogate models, offering a promising direction for reducing computational costs in expensive optimization problems. |
| [LICO: Large Language Models for In-Context Molecular Optimization](https://doi.org/10.48550/arXiv.2406.18851) | ArXiv | Jun 2024 | LICO extends pretrained LLMs for black-box molecular optimization by equipping them with domain-specific embedding layers and training on semi-synthetic functions (intrinsic properties + Gaussian Processes). It achieves state-of-the-art performance on the PMO benchmark through in-context learning, demonstrating LLMs' potential in scientific domains with limited data. The method highlights the critical role of language-guided embeddings and diversified training functions for generalization. |
| [Large Language Model-assisted Surrogate Modelling for Engineering Optimization](https://doi.org/10.1109/CAI59869.2024.00151) | CAI | Jun 2024 | This paper proposes a ChatGPT 4-assisted framework for selecting and training surrogate models in engineering optimization, demonstrating that LLM-generated workflows achieve performance comparable to conventional methods in both synthetic benchmarks and real-world automotive design tasks. The study highlights LLMs' potential to democratize data science expertise while noting limitations in extrapolation and data preprocessing. |

#### 2.5 LLMs for Bayesian Optimization
**Title** | **Publication** | **Year** | **Summary**
--- | --- | --- | ---
[Large Language Models to Enhance Bayesian Optimization](https://arxiv.org/pdf/2402.03921) | ICLR | Mar 2024 | LLAMBO integrates large language models (LLMs) with Bayesian optimization (BO) through natural language prompting, enhancing warm-starting, surrogate modeling, and candidate sampling via in-context learning. It achieves superior sample efficiency in hyperparameter tuning, particularly in low-data regimes, while maintaining modularity for seamless integration into existing BO frameworks.
[LLM-Enhanced Bayesian Optimization for Efficient Analog Layout Constraint Generation](https://doi.org/10.48550/arXiv.2406.05250) | ArXiv | Jun 2024 | LLANA introduces an LLM-enhanced Bayesian Optimization framework that leverages few-shot learning and contextual generation to efficiently explore analog layout constraints. Experiments demonstrate its superiority over GP and SMAC in sample efficiency and uncertainty calibration, enabling faster convergence with limited data.
[FunBO: Discovering Acquisition Functions for Bayesian Optimization with FunSearch](https://arxiv.org/pdf/2406.04824) | ArXiv | Jun 2024 | FunBO leverages LLMs to automatically discover interpretable acquisition functions via code evolution, achieving superior sample efficiency and generalization across diverse Bayesian optimization tasks, both in-distribution and out-of-distribution. This approach demonstrates the potential of combining evolutionary algorithms with large language models for algorithm design in mathematical sciences.
[Evolve Cost-aware Acquisition Functions Using Large Language Models](https://arxiv.org/pdf/2404.16906) | PPSN | Jun 2024 | The paper introduces EvolCAF, a framework integrating LLMs and evolutionary computation to automatically design cost-aware acquisition functions for Bayesian optimization. By leveraging LLMs for code generation and evolution, EvolCAF reduces expert dependency and achieves superior performance over human-crafted methods (e.g., EIpu, EI-cool) across synthetic and real-world tasks.
[ADO-LLM: Analog Design Bayesian Optimization with In-Context Learning of Large Language Models](https://arxiv.org/pdf/2406.18770) | ArXiv | Jun 2024 | ADO-LLM integrates Large Language Models (LLMs) with Bayesian Optimization (BO) to automate analog circuit design, leveraging LLMs' domain knowledge for rapid initial design generation and BO's exploration for diversity. By combining in-context learning with high-quality demonstrations from BO, the framework achieves superior efficiency and effectiveness, validated on multiple circuits with all specifications met.
[Searching for Optimal Solutions with LLMs via Bayesian Optimization](https://openreview.net/pdf?id=aVfDrl7xDV) | ICLR | Jan 2025 | BOPRO integrates Bayesian optimization with LLMs to dynamically balance exploration-exploitation in solution search. It outperforms baselines in word and molecule optimization but faces limitations in program synthesis due to coarse-grained code representations.
[Language-Based Bayesian Optimization Research Assistant (BORA)](https://arxiv.org/pdf/2501.16224) | ArXiv | Jan 2025 | BORA integrates large language models with Bayesian optimization to dynamically inject domain knowledge through hypothesis generation and real-time commentary. By adaptively switching between LLM-guided exploration and BO-driven exploitation, it achieves superior performance in high-dimensional scientific tasks, validated by synthetic benchmarks and real-world experiments in chemistry and energy.



### 3. High-level LLM-assisted Optimization Algorithms

#### 3.1 Algorithm Selection

| **Title**                                                                                      | **Publication** | **Year**  | **Summary**                                                                                                                                                                                                                                                                                               |
|--------------------------------------------------------------------------------------------|-------------|-------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| [AS-LLM: When Algorithm Selection Meets Large Language Models](https://openreview.net/forum?id=l7aD9VMQUq) | Openreview  | Sep 2023 | This paper proposes an algorithm selection framework that integrates algorithm representation into the selection process, leveraging pre-trained Large Language Models (LLMs) for code comprehension. The approach demonstrates improved selection accuracy by using matching degree calculations between extracted algorithm and problem embeddings, and showcases the potential of LLMs in evaluating code representation. |
| [OptiMUS: Optimization Modeling Using MIP Solvers and Large Language Models](https://arxiv.org/abs/2310.06116) | ArXiv       | Oct 2023 | OptiMUS generates Python code from problem formulations to read input, call optimization solvers like Gurobi and cvxpy, and save the output. It utilizes advanced solver features and includes solver-specific instructions to correct common code errors and improve performance. |
| [Large Language Model-Enhanced Algorithm Selection: Towards Comprehensive Algorithm Representation](https://arxiv.org/abs/2311.13184) | IJCAI      | Nov 2023 | This paper introduces Large Language Models (LLMs) into the algorithm selection process for automated machine learning, bridging the gap in algorithm feature representation. The proposed model, which combines high-dimensional algorithm representations with problem features, outperforms existing techniques and provides valuable theoretical insights for practical implementation. |
| [OptiMUS: Scalable Optimization Modeling with (MI)LP Solvers and Large Language Models](https://doi.org/10.48550/arXiv.2402.10172) | ICML     | Feb 2024 | OptiMUS introduces a modular LLM-based agent framework that automates optimization modeling by integrating natural language processing with MILP solvers. It achieves state-of-the-art performance through connection graphs for context efficiency and multi-agent collaboration, outperforming prior methods by over 30% on complex datasets like NLP4LP. |
| [OptiMUS-0.3: Using Large Language Models to Model and Solve Optimization Problems at Scale](https://doi.org/10.48550/arXiv.2407.19633) | ArXiv     | Jul 2024 | OptiMUS-0.3 leverages modular LLM agents to automate optimization modeling, translating natural language problems into MILP formulations and solver code. It introduces self-corrective prompts, structure-aware modeling, and a novel dataset (NLP4LP), outperforming state-of-the-art methods by over 24% on complex tasks while enabling scalable human-in-the-loop validation. |

#### 3.2 Algorithm Generation

We further divide the "Algorithm Generation" process into two approaches: Single-round Generation and Iterative Generation. Single-round Generation involves generating a complete algorithm in a single interaction with the model, without further modifications. In contrast, Iterative Generation entails multiple rounds of refinement, where the model's output from each round serves as the input for the next, progressively improving the algorithm. 

#### 3.2.0 Benchmark and Plafrorm

| **Title** | **Publication** | **Year** |
|-------|-------------|------|
 [LLM4AD: A Platform for Algorithm Design with Large Language Model](https://arxiv.org/pdf/2412.17287) | ArXiv | Dec 2024 |
| [Beyond the Hype: Benchmarking LLM-Evolved Heuristics for Bin Packing](https://arxiv.org/pdf/2501.11411) | EvoApplications | Jan 2025 |
| [BLADE: Benchmark suite for LLM-driven Automated Design and Evolution of iterative optimisation heuristics](https://arxiv.org/pdf/2504.20183) | ArXiv | Apr 2025 |

#### 3.2.1 Single-round Generation

| **Title** | **Publication** | **Year** | **Summary** |
|-------|-------------|------|---------|
| [Leveraging Large Language Models for the Generation of Novel Metaheuristic Optimization Algorithms](https://dl.acm.org/doi/abs/10.1145/3583133.3596401) | GECCO | Jul 2023 | This paper explores the use of Large Language Models (LLMs) like GPT-4 to generate novel hybrid swarm intelligence optimization algorithms by decomposing six well-established algorithms for continuous optimization. The study focuses on the hybridization process, the challenges encountered, and the potential impact of LLM-generated algorithms in the metaheuristics field, offering insights into future research directions. |
| [Leveraging large language model to generate a novel metaheuristic algorithm with CRISPE framework](https://link.springer.com/article/10.1007/s10586-024-04654-6) | Cluster Computing | Apr 2024 | This paper introduces a novel metaheuristic algorithm, Zoological Search Optimization (ZSO), generated by ChatGPT-3.5 using the CRISPE framework, inspired by animal collective behaviors for continuous optimization. Experimental results on benchmark functions and engineering problems demonstrate the efficiency and effectiveness of ZSO-derived algorithms, highlighting the potential impact of Large Language Models (LLMs) in advancing the metaheuristics community. |

#### 3.2.2 Iterative Generation

| **Title** | **Publication** | **Year** | **Summary** |
|---|---|---|---|
| [Mathematical discoveries from program search with large language models](https://www.nature.com/articles/s41586-023-06924-6) | Nature | Aug 2023 | FunSearch pioneers a novel AI-driven discovery paradigm by combining large language models with evolutionary algorithms to solve open mathematical problems and design optimized algorithms. This approach not only generates verifiable new knowledge in combinatorics (e.g., improved cap set constructions) but also produces human-interpretable programs that outperform classical heuristics in practical optimization challenges like online bin packing. |
| [Algorithm Evolution Using Large Language Model](https://arxiv.org/pdf/2311.15249) | ArXiv | Nov 2023 | AEL leverages LLMs within an evolutionary framework to automatically generate and optimize algorithms, demonstrating superior scalability and performance on TSP compared to human-designed heuristics and domain-specific models. By evolving algorithm-level strategies through crossover and mutation powered by LLMs, AEL reduces manual effort and achieves state-of-the-art generalization across problem scales. |
| [Evolution of Heuristics: Towards Efficient Automatic Algorithm Design Using Large Language Model](https://arxiv.org/abs/2401.02051) | ICML | Jan 2024 | This paper introduces Evolution of Heuristics (EoH), an innovative method combining Large Language Models (LLMs) and Evolutionary Computation (EC) for Automatic Heuristic Design (AHD). EoH evolves both heuristic ideas (thoughts) in natural language and their corresponding executable codes, enhancing efficiency and effectiveness in generating high-performance heuristics. |
| [ReEvo: Large Language Models as Hyper-Heuristics with Reflective Evolution](https://proceedings.neurips.cc/paper_files/paper/2024/file/4ced59d480e07d290b6f29fc8798f195-Paper-Conference.pdf) | NeurIPS | Feb 2024 | ReEvo integrates large language models with reflective evolution to automate heuristic design for combinatorial optimization, where LLMs generate and refine heuristics through evolutionary search coupled with verbal feedback, achieving state-of-the-art performance across diverse problems with enhanced sample efficiency and generalization. |
| [AutoSAT: Automatically Optimize SAT Solvers via Large Language Models](https://arxiv.org/pdf/2402.10705) | ArXiv | Feb 2024 | AutoSAT automates the optimization of CDCL SAT solvers by leveraging LLMs to refine heuristic functions within a modular framework. It outperforms MiniSat on 9/12 datasets and surpasses Kissat on 4, demonstrating LLMs' potential in complex algorithm design through evolutionary-guided code generation. |
| [LLaMEA: A Large Language Model Evolutionary Algorithm for Automatically Generating Metaheuristics](https://arxiv.org/abs/2405.20132) | TEVC | May 2024 | LLaMEA automates metaheuristic design by integrating large language models (LLMs) with evolutionary algorithms, where GPT-4 iteratively generates and refines optimization codes based on performance feedback from BBOB benchmarks. The framework produces novel algorithms (e.g., ERADS) that outperform state-of-the-art methods in 5D and show scalability to higher dimensions, demonstrating the potential of LLMs as modular algorithm synthesizers. |
| [L-AutoDA: Large Language Models for Automatically Evolving Decision-based Adversarial Attacks](https://arxiv.org/pdf/2401.15335) | GECCO | May 2024 | This paper introduces L-AutoDA, a framework that leverages large language models (LLMs) based on AEL (Algorithm Evolution Using Large Language Model) to automatically generate decision-based adversarial attack algorithms through evolutionary optimization, demonstrating superior performance over manual designs on CIFAR-10. |
| [Autonomous Multi-Objective Optimization Using Large Language Model](https://doi.org/10.48550/arXiv.2406.08987) | ArXiv | Jun 2024 | The paper proposes LLMOPT, a framework that leverages large language models (LLMs) to autonomously design evolutionary algorithm operators for multi-objective optimization. By integrating error-driven repair, dynamic selection, and LLM-powered crossover/mutation, LLMOPT generates robust operators that outperform traditional methods on diverse continuous and combinatorial optimization problems, reducing reliance on expert intervention. |
| [Understanding the Importance of Evolutionary Search in Automated Heuristic Design with Large Language Models](https://arxiv.org/abs/2407.10873) | PPSN | Jul 2024 | This study establishes the necessity of integrating evolutionary search with LLMs for automated heuristic design (AHD), demonstrating through large-scale benchmarks that standalone LLMs underperform evolutionary program search (EPS) methods like (1+1)-EPS and EoH across combinatorial optimization tasks, while revealing significant performance dependencies on problem types and LLM selection (e.g., code-specialized models outperform general-purpose ones in TSP). |
| [TS-EoH: An Edge Server Task Scheduling Algorithm Based on Evolution of Heuristic](https://arxiv.org/abs/2409.09063) | ArXiv | Sep 2024 | This paper presents a novel task-scheduling approach using Evolutionary Computing (EC) and heuristic algorithms to address the challenges of low-latency real-time processing in edge computing. By modeling service requests as task sequences and evaluating scheduling schemes through Large Language Models (LLMs), the proposed algorithm outperforms traditional methods, including heuristic and reinforcement learning approaches. |
| [Multi-objective Evolution of Heuristic Using Large Language Model](https://doi.org/10.48550/arXiv.2409.16867) | AAAI | Sep 2024 | The paper proposes MEoH, a multi-objective evolutionary framework leveraging large language models (LLMs) to automatically design heuristics that balance optimality and efficiency. By integrating dominance relationships in objective space and code dissimilarity via AST-based metrics, MEoH generates diverse non-dominated heuristics, achieving up to 10Ã— speedup while maintaining competitive performance in combinatorial optimization tasks like bin packing and traveling salesman problems. |
| [In-the-loop Hyper-Parameter Optimization for LLM-Based Automated Design of Heuristics](https://arxiv.org/pdf/2410AAAI.16309) | ArXiv | Oct 2024 | The paper proposes LLaMEA-HPO, a hybrid framework integrating LLMs with hyper-parameter optimization (HPO), where LLMs focus on generating novel algorithmic structures while SMAC3 handles parameter tuning. This approach reduces LLM query costs by 80% and achieves state-of-the-art performance in heuristic design for problems like BBOB and TSP. |
| [Controlling the Mutation in Large Language Models for the Efficient Evolution of Algorithms](https://arxiv.org/pdf/2412.03250) | EvoApplications | Dec 2024 | This paper introduces â€‹â€‹dynamic mutation promptsâ€‹â€‹ with power-law-distributed rates to enhance LLM-driven algorithm evolution, showing that â€‹â€‹GPT-4 outperforms GPT-3.5â€‹â€‹ in adhering to mutation controls and improving convergence speed in LLaMEA. |
| [HSEvo: Elevating Automatic Heuristic Design with Diversity-Driven Harmony Search and Genetic Algorithm Using LLMs](https://arxiv.org/abs/2412.14995) | AAAI | Dec 2024 | This study explores the use of Large Language Models (LLMs) combined with evolutionary computation for automatic heuristic design (AHD), referred to as LLM-based Evolutionary Program Search (LLM-EPS). While previous methods like FunSearch, EoH, and ReEvo have shown promise, challenges remain in balancing exploration and exploitation in large heuristic search spaces. The authors propose two diversity measurement metrics to address this gap and introduce HSEvo, an adaptive framework that maintains this balance through a harmony search algorithm, achieving both high diversity and good objective scores efficiently. |
| [Monte Carlo Tree Search for Comprehensive Exploration in LLM-Based Automatic Heuristic Design](https://doi.org/10.48550/arXiv.2501.08603) | ArXiv | Jan 2025 | MCTS-AHD integrates Monte Carlo Tree Search with LLMs to automate heuristic design, enabling comprehensive exploration of heuristic spaces by retaining and refining underperforming candidates through tree-structured evolution. It outperforms population-based methods on NP-hard optimization tasks by balancing exploration-exploitation via decayed exploration factors and progressive widening. |
| [Complex LLM Planning via Automated Heuristics Discovery](https://arxiv.org/pdf/2502.19295) | ArXiv | Feb 2025 | AutoHD enables LLMs to generate explicit heuristic functions for planning tasks, which guide efficient search via algorithms like A* and evolve through iterative refinement. It achieves state-of-the-art performance across benchmarks (e.g., nearly doubled accuracy on some datasets) without additional training, offering both reliability and interpretability in complex reasoning. |
| [LLM-Assisted Automatic Memetic Algorithm for Lot-Streaming Hybrid Job Shop Scheduling With Variable Sublots](https://doi.org/10.1109/TEVC.2025.3556186) | TEVC | Mar 2025 | This paper proposes an LLM-assisted memetic algorithm (LLMMA) that automatically designs heuristics for solving complex lot-streaming hybrid job shop scheduling with variable sublots (LHJSV), demonstrating superior performance over traditional methods. |
| [Fitness Landscape of Large Language Model-Assisted Automated Algorithm Search](https://arxiv.org/pdf/2504.19636) | ArXiv | May 2025 | The paper introduces a â€‹â€‹graph-based fitness landscape analysisâ€‹â€‹ to characterize the behavior of â€‹â€‹LLM-assisted algorithm search (LAS)â€‹â€‹, revealing its â€‹â€‹multimodal, rugged natureâ€‹â€‹ and task-dependent variations across different LLMs and search settings. This provides â€‹â€‹practical insightsâ€‹â€‹ for optimizing automated algorithm design. |

### 4. Other Studies

#### 4.1 Autoformulation
| **Title** | **Publication** | **Year** | **Summary** |
|-----------|---------------|----------|------------|
| [OptiMUS: Optimization Modeling Using MIP Solvers and Large Language Models](https://arxiv.org/abs/2310.06116) | ArXiv | Oct 2023 | OptiMUS generates Python code from problem formulations to read input, call optimization solvers like Gurobi and cvxpy, and save the output. It utilizes advanced solver features and includes solver-specific instructions to correct common code errors and improve performance. |
| [LM4OPT: Unveiling the potential of Large Language Models in formulating mathematical optimization problems](https://www.tandfonline.com/doi/abs/10.1080/03155986.2024.2388452) | INFOR | Dec 2023 | This study evaluates GPT-4, GPT-3.5, and Llama-2-7b in translating natural language descriptions into mathematical optimization formulations, revealing GPT-4's superiority in one-shot settings (F1-score 0.63) and proposing the LM4OPT framework to enhance smaller models through progressive fine-tuning with noisy embeddings. While GPT models outperformed fine-tuned Llama-2-7b, the research highlights limitations of smaller LLMs in handling complex contexts compared to larger counterparts. |
| [OptiMUS: Scalable Optimization Modeling with (MI)LP Solvers and Large Language Models](https://doi.org/10.48550/arXiv.2402.10172) | ICML | Feb 2024 | OptiMUS introduces a modular LLM-based agent framework that automates optimization modeling by integrating natural language processing with MILP solvers. It achieves state-of-the-art performance through connection graphs for context efficiency and multi-agent collaboration, outperforming prior methods by over 30% on complex datasets like NLP4LP. |
| [OptiMUS-0.3: Using Large Language Models to Model and Solve Optimization Problems at Scale](https://doi.org/10.48550/arXiv.2407.19633) | ArXiv | Jul 2024 | OptiMUS-0.3 leverages modular LLM agents to automate optimization modeling, translating natural language problems into MILP formulations and solver code. It introduces self-corrective prompts, structure-aware modeling, and a novel dataset (NLP4LP), outperforming state-of-the-art methods by over 24% on complex tasks while enabling scalable human-in-the-loop validation. |
| [LLMOPT: Learning to Define and Solve General Optimization Problems from Scratch](https://doi.org/10.48550/arXiv.2410.13213) | ICLR | Oct 2024 | The study proposes LLMOPT, a framework enhancing LLMs' generalization in solving diverse optimization problems (e.g., linear/nonlinear programming) by introducing a five-element formulation for unified problem representation and leveraging multi-instruction fine-tuning with model alignment. It achieves 11.08% higher accuracy than state-of-the-art methods across 20+ domains, demonstrating robust automation in optimization modeling and solving. |
| [Autoformulation of Mathematical Optimization Models Using LLMs](https://doi.org/10.48550/arXiv.2411.01679) | ArXiv | Nov 2024 | This paper introduces an LLM-enhanced framework for autoformulation of optimization models, leveraging Monte-Carlo Tree Search (MCTS) to systematically explore hierarchical components (variables, objectives, constraints) while pruning redundant hypotheses via SMT solvers. LLMs dynamically generate context-aware formulations and evaluate correctness, achieving superior performance in benchmarks through combined semantic and solver-based feedback. |
| [Enhancing healthcare resource allocation through large language models](https://doi.org/10.1016/j.swevo.2025.101859) | SEC | Nov 2024 | This study proposes an LLM-based framework integrating Prompt Engineering, RAG, and tool utilization to optimize healthcare resource allocation dynamically. ChatGPT-4o outperformed traditional methods by reducing costs and overtime while adapting to scale changes without re-optimization, demonstrating LLMs' potential in balancing multi-objective constraints and real-world medical complexities. |
| [Bridging Large Language Models and Optimization: A Unified Framework for Text-attributed Combinatorial Optimization](https://doi.org/10.48550/arXiv.2408.12214) | ArXiv | Dec 2024 | LNCS bridges LLMs and combinatorial optimization by encoding text-attributed problems into a unified semantic space and generating solutions via a Transformer-based network. Trained with conflict-free multi-task reinforcement learning, it achieves state-of-the-art performance across diverse COPs while enabling cross-problem generalization through fine-tuning. |
| [Meta-Optimization and Program Search using Language Models for Task and Motion Planning](https://arxiv.org/pdf/2505.03725) | ArXiv | May 2024 | The paper introduces â€‹â€‹MOPSâ€‹â€‹, a hierarchical method combining â€‹â€‹language modelsâ€‹â€‹ and â€‹â€‹numerical optimizationâ€‹â€‹ to solve â€‹â€‹task and motion planning (TAMP)â€‹â€‹ by searching over constraint sequences instead of action sequences, enabling precise robotic manipulation from natural language instructions. |

## How to Contribute

If you would like to add more papers to this collection or suggest edits, please feel free to open a pull request. Contributions are welcome! Please follow these guidelines:

1. Fork this repository.
2. Add your paper to the appropriate section (either Problem-driven LLM-based Optimization, Low-level LLM-assisted EAs, or High-level LLM-assisted EAs).
3. Add a brief summary of the paper along with the title, authors, and year.
4. Submit a pull request for review.